import polars as pl
from pathlib import Path
import multiprocessing
import time
import shutil
import re

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BASE_PATH = Path(r"D:\HakatonNew\Dataset\Dataset\1000k\telecom1000k")
TEMP_DIR = Path("temp_processing")
OUTPUT_FILE = "result_dispersia_time15.csv"

# –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è –Ω–æ—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (23:00-06:00)
NIGHT_START = 23
NIGHT_END = 6


def init_temp_dir():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    TEMP_DIR.mkdir(exist_ok=True)


def clean_temp_dir():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)


def extract_time_from_filename(filename):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    match = re.search(r'(\d{2})_(\d{2})_(\d{2})', filename.stem)
    if match:
        return int(match.group(1))  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
    return None


def is_night_time(hour):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –≤—Ä–µ–º—è –∫ –Ω–æ—á–Ω–æ–º—É"""
    if hour is None:
        return False
    return hour >= NIGHT_START or hour < NIGHT_END


def process_psx_file(file_path):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ PSX —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª"""
    try:
        temp_file = TEMP_DIR / f"processed_{file_path.stem}.parquet"
        hour = extract_time_from_filename(file_path)
        time_type = "night" if is_night_time(hour) else "day"

        if file_path.suffix == '.csv':
            df = pl.read_csv(file_path, separator=',', infer_schema_length=10000)
        elif file_path.suffix == '.txt':
            df = pl.read_csv(file_path, separator='|', infer_schema_length=10000)
        else:
            return None

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö - –∏—Å–∫–ª—é—á–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.filter(
            (pl.col("UpTx") >= 0) &
            (pl.col("DownTx") >= 0))

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫
        df = df.with_columns(pl.lit(time_type).alias("TimeType"))

        df.select(["IdSubscriber", "UpTx", "DownTx", "TimeType"]).write_parquet(temp_file)
        return temp_file

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path.name}: {str(e)[:100]}...")
        return None


def calculate_iqr_thresholds(df):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º IQR —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫"""
    return (
        df.group_by(["Type", "TimeType"])
        .agg(
            pl.quantile("ratio", 0.25).alias("q1"),
            pl.quantile("ratio", 0.75).alias("q3")
        )
        .with_columns(
            (pl.col("q3") - pl.col("q1")).alias("iqr"),
            (pl.col("q3") + 2.5 * (pl.col("q3") - pl.col("q1"))).alias("upper_threshold")
        )
    )


def load_and_process_data():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    init_temp_dir()

    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
        print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤...")
        subs = pl.read_csv(BASE_PATH / "subscribers.csv").filter(
            pl.col("Status") == "ON"
        ).select(["IdOnPSX", "IdClient"])

        # 2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ PSX
        psx_files = list(BASE_PATH.glob("psx_*.*"))
        if not psx_files:
            raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã psx_*")

        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(psx_files)} —Ñ–∞–π–ª–æ–≤ —Ç—Ä–∞—Ñ–∏–∫–∞...")

        with multiprocessing.Pool(processes=min(4, multiprocessing.cpu_count())) as pool:
            processed_files = pool.map(process_psx_file, psx_files)

        psx_dfs = [pl.read_parquet(f) for f in processed_files if f]
        if not psx_dfs:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞—Ñ–∏–∫–∞")

        psx = pl.concat(psx_dfs)

        # 3. –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ç—Ä–∞—Ñ–∏–∫–∞ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        print("üßÆ –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞...")
        traffic = psx.group_by(["IdSubscriber", "TimeType"]).agg(
            pl.sum("UpTx").alias("sumUpTx"),
            pl.sum("DownTx").alias("sumDownTx")
        ).filter(pl.col("sumDownTx") > 0)

        # 4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö
        print("üë• –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤...")
        clients_df = pl.read_parquet(BASE_PATH / "client.parquet").select(["Id", "IdPlan"])
        physical_df = pl.read_parquet(BASE_PATH / "physical.parquet").select("Id")

        # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–ª–∏–µ–Ω—Ç–∞ –∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        print("üîó –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–ª–∏–µ–Ω—Ç–∞...")
        client_types = (
            subs.select("IdClient")
            .join(
                physical_df.rename({"Id": "IdClient"}).with_columns(pl.lit("P").alias("Type")),
                on="IdClient",
                how="left"
            )
            .with_columns(pl.col("Type").fill_null("J"))
            .unique()
        )

        joined = (
            subs.join(traffic, left_on="IdOnPSX", right_on="IdSubscriber", how="inner")
            .join(client_types, on="IdClient", how="left")
            .join(clients_df, left_on="IdClient", right_on="Id", how="left")
            .with_columns(
                (pl.col("sumUpTx") / pl.col("sumDownTx")).alias("ratio")
            )
            .filter(pl.col("sumDownTx") > 0)
        )

        # 6. –†–∞—Å—á–µ—Ç IQR –ø–æ—Ä–æ–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫
        print("üìä –†–∞—Å—á–µ—Ç IQR –ø–æ—Ä–æ–≥–æ–≤ –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π...")
        iqr_stats = calculate_iqr_thresholds(joined)

        # 7. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        print("üîé –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π...")
        anomalies = (
            joined.join(iqr_stats, on=["Type", "TimeType"])
            .filter(pl.col("ratio") > pl.col("upper_threshold"))
        )

        # 8. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        result = (
            anomalies.select(
                pl.col("IdOnPSX").alias("Id"),
                pl.col("IdClient").alias("UID"),
                pl.col("Type"),
                pl.col("IdPlan").alias("IdPlan"),
                pl.lit(True).alias("TurnOn"),
                pl.lit(True).alias("Hacked"),
                pl.lit(0).alias("Traffic")
            )
            .unique(subset=["Id"])  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ Id
        )
        # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {result.height} –∞–Ω–æ–º–∞–ª–∏–π –≤ {OUTPUT_FILE}...")
        result.write_csv(OUTPUT_FILE)
        print("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        #print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π:")
        #print(result.group_by(["Type", "Period"]).agg(pl.len()))

        return True

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}: {str(e)}")
        return False
    finally:
        clean_temp_dir()


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
    start_time = time.time()

    success = load_and_process_data()

    elapsed = time.time() - start_time
    print(f"‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
    exit(0 if success else 1)